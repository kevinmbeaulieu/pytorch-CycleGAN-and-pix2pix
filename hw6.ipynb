{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torchvision import datasets\n",
    "\n",
    "train_dataset = datasets.MNIST('datasets/', train=True, download=True)\n",
    "test_dataset = datasets.MNIST('datasets/', train=False, download=True)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    train_dataset.data,\n",
    "    train_dataset.targets,\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    test_dataset.data,\n",
    "    test_dataset.targets,\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def save_images(dataset, path_A, path_B):\n",
    "    if os.path.exists(path_A) and os.path.exists(path_B):\n",
    "        return\n",
    "\n",
    "    os.makedirs(path_A)\n",
    "    os.makedirs(path_B)\n",
    "\n",
    "    def clear_directory(path):\n",
    "        files = glob.glob(path + '*')\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "    clear_directory(path_A)\n",
    "    clear_directory(path_B)\n",
    "\n",
    "    images = [\n",
    "        (Image.fromarray(image.numpy().astype('uint8')), int(label))\n",
    "        for image, label in dataset\n",
    "    ]\n",
    "    images_by_label = {}\n",
    "    for image, label in images:\n",
    "        if label not in images_by_label:\n",
    "            images_by_label[label] = []\n",
    "        images_by_label[label].append(image)\n",
    "\n",
    "    num_01 = min(len(images_by_label[0]), len(images_by_label[1]))\n",
    "    num_23 = min(len(images_by_label[2]), len(images_by_label[3]))\n",
    "    num_45 = min(len(images_by_label[4]), len(images_by_label[5]))\n",
    "    num_67 = min(len(images_by_label[6]), len(images_by_label[7]))\n",
    "    num_89 = min(len(images_by_label[8]), len(images_by_label[9]))\n",
    "\n",
    "    start_index = 0\n",
    "    for i in range(num_01):\n",
    "        images_by_label[0][i].save(os.path.join(path_A, f'{start_index+i}.png'))\n",
    "        images_by_label[1][i].save(os.path.join(path_B, f'{start_index+i}.png'))\n",
    "    start_index += num_01\n",
    "    for i in range(num_23):\n",
    "        images_by_label[2][i].save(os.path.join(path_A, f'{start_index+i}.png'))\n",
    "        images_by_label[3][i].save(os.path.join(path_B, f'{start_index+i}.png'))\n",
    "    start_index += num_23\n",
    "    for i in range(num_45):\n",
    "        images_by_label[4][i].save(os.path.join(path_A, f'{start_index+i}.png'))\n",
    "        images_by_label[5][i].save(os.path.join(path_B, f'{start_index+i}.png'))\n",
    "    start_index += num_45\n",
    "    for i in range(num_67):\n",
    "        images_by_label[6][i].save(os.path.join(path_A, f'{start_index+i}.png'))\n",
    "        images_by_label[7][i].save(os.path.join(path_B, f'{start_index+i}.png'))\n",
    "    start_index += num_67\n",
    "    for i in range(num_89):\n",
    "        images_by_label[8][i].save(os.path.join(path_A, f'{start_index+i}.png'))\n",
    "        images_by_label[9][i].save(os.path.join(path_B, f'{start_index+i}.png'))\n",
    "\n",
    "save_images(train_dataset, 'datasets/mnist/A/train/', 'datasets/mnist/B/train/')\n",
    "save_images(val_dataset, 'datasets/mnist/A/val/', 'datasets/mnist/B/val/')\n",
    "save_images(test_dataset, 'datasets/mnist/A/test/', 'datasets/mnist/B/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold_A] =  datasets/mnist/A\n",
      "[fold_B] =  datasets/mnist/B\n",
      "[fold_AB] =  datasets/mnist/AB\n",
      "[num_imgs] =  1000000\n",
      "[use_AB] =  False\n",
      "[no_multiprocessing] =  False\n",
      "split = test, use 4814/4814 images\n",
      "split = test, number of images = 4814\n",
      "split = train, use 24179/24179 images\n",
      "split = train, number of images = 24179\n",
      "split = val, use 4856/4856 images\n",
      "split = val, number of images = 4856\n"
     ]
    }
   ],
   "source": [
    "!python datasets/combine_A_and_B.py --fold_A datasets/mnist/A --fold_B datasets/mnist/B --fold_AB datasets/mnist/AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sp7TCT2x9dB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./datasets/mnist/AB           \t[default: None]\n",
      "             dataset_mode: aligned                       \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: -1                            \t[default: 1]\n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: vanilla                       \n",
      "                  gpu_ids: -1                            \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                lambda_L1: 100.0                         \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: 1000                          \t[default: inf]\n",
      "                    model: pix2pix                       \t[default: cycle_gan]\n",
      "                 n_epochs: 30                            \t[default: 100]\n",
      "           n_epochs_decay: 30                            \t[default: 100]\n",
      "               n_layers_D: 3                             \n",
      "                     name: mnist_pix2pix                 \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: True                          \t[default: False]\n",
      "                  no_html: False                         \n",
      "                     norm: batch                         \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 1000                          \t[default: 5000]\n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "dataset [AlignedDataset] was created\n",
      "The number of training images = 1000\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 54.414 M\n",
      "[Network D] Total number of parameters : 2.769 M\n",
      "-----------------------------------------------\n",
      "create web directory ./checkpoints/mnist_pix2pix/web...\n",
      "/Users/kevinbeaulieu/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 1, iters: 100, time: 0.545, data: 3.274) G_GAN: 4.335 G_L1: 30.280 D_real: 0.031 D_fake: 0.015 \n",
      "(epoch: 1, iters: 200, time: 0.557, data: 0.000) G_GAN: 4.940 G_L1: 33.741 D_real: 0.021 D_fake: 0.010 \n",
      "(epoch: 1, iters: 300, time: 0.553, data: 0.000) G_GAN: 3.792 G_L1: 44.885 D_real: 0.041 D_fake: 0.044 \n",
      "(epoch: 1, iters: 400, time: 0.623, data: 0.000) G_GAN: 1.945 G_L1: 23.808 D_real: 0.116 D_fake: 0.503 \n",
      "(epoch: 1, iters: 500, time: 0.548, data: 0.000) G_GAN: 1.433 G_L1: 24.116 D_real: 0.041 D_fake: 1.175 \n",
      "(epoch: 1, iters: 600, time: 0.590, data: 0.000) G_GAN: 1.823 G_L1: 22.616 D_real: 0.042 D_fake: 0.342 \n",
      "(epoch: 1, iters: 700, time: 0.592, data: 0.000) G_GAN: 1.599 G_L1: 17.672 D_real: 0.328 D_fake: 0.259 \n",
      "(epoch: 1, iters: 800, time: 0.599, data: 0.000) G_GAN: 1.916 G_L1: 32.408 D_real: 0.038 D_fake: 0.352 \n",
      "(epoch: 1, iters: 900, time: 0.587, data: 0.000) G_GAN: 1.663 G_L1: 32.094 D_real: 0.038 D_fake: 0.219 \n",
      "(epoch: 1, iters: 1000, time: 0.556, data: 0.000) G_GAN: 2.091 G_L1: 22.045 D_real: 0.137 D_fake: 0.124 \n",
      "saving the latest model (epoch 1, total_iters 1000)\n",
      "End of epoch 1 / 60 \t Time Taken: 601 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 2, iters: 100, time: 0.564, data: 4.221) G_GAN: 1.943 G_L1: 26.840 D_real: 0.027 D_fake: 0.136 \n",
      "(epoch: 2, iters: 200, time: 0.638, data: 0.000) G_GAN: 0.904 G_L1: 36.535 D_real: 0.006 D_fake: 1.460 \n",
      "(epoch: 2, iters: 300, time: 0.575, data: 0.000) G_GAN: 0.964 G_L1: 17.303 D_real: 1.417 D_fake: 0.092 \n",
      "(epoch: 2, iters: 400, time: 0.544, data: 0.000) G_GAN: 0.744 G_L1: 19.678 D_real: 1.863 D_fake: 0.681 \n",
      "(epoch: 2, iters: 500, time: 0.574, data: 0.000) G_GAN: 1.414 G_L1: 32.377 D_real: 0.004 D_fake: 0.581 \n",
      "(epoch: 2, iters: 600, time: 0.561, data: 0.000) G_GAN: 1.290 G_L1: 41.605 D_real: 0.007 D_fake: 0.516 \n",
      "(epoch: 2, iters: 700, time: 0.568, data: 0.000) G_GAN: 2.322 G_L1: 14.559 D_real: 1.375 D_fake: 0.044 \n",
      "(epoch: 2, iters: 800, time: 0.567, data: 0.000) G_GAN: 2.535 G_L1: 19.403 D_real: 0.337 D_fake: 0.315 \n",
      "(epoch: 2, iters: 900, time: 0.589, data: 0.000) G_GAN: 1.691 G_L1: 34.843 D_real: 0.082 D_fake: 0.880 \n",
      "(epoch: 2, iters: 1000, time: 0.609, data: 0.000) G_GAN: 2.269 G_L1: 39.917 D_real: 0.014 D_fake: 0.221 \n",
      "saving the latest model (epoch 2, total_iters 2000)\n",
      "End of epoch 2 / 60 \t Time Taken: 621 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 3, iters: 100, time: 0.580, data: 4.113) G_GAN: 0.877 G_L1: 25.557 D_real: 0.960 D_fake: 0.749 \n",
      "(epoch: 3, iters: 200, time: 0.570, data: 0.000) G_GAN: 2.564 G_L1: 26.186 D_real: 0.552 D_fake: 0.067 \n",
      "(epoch: 3, iters: 300, time: 0.564, data: 0.000) G_GAN: 1.978 G_L1: 26.023 D_real: 0.170 D_fake: 0.241 \n",
      "(epoch: 3, iters: 400, time: 0.630, data: 0.000) G_GAN: 1.901 G_L1: 30.611 D_real: 0.040 D_fake: 0.302 \n",
      "(epoch: 3, iters: 500, time: 0.589, data: 0.000) G_GAN: 2.434 G_L1: 13.511 D_real: 1.572 D_fake: 0.024 \n",
      "(epoch: 3, iters: 600, time: 0.578, data: 0.000) G_GAN: 1.026 G_L1: 28.242 D_real: 1.278 D_fake: 0.740 \n",
      "(epoch: 3, iters: 700, time: 0.564, data: 0.000) G_GAN: 1.296 G_L1: 25.578 D_real: 0.795 D_fake: 0.220 \n",
      "(epoch: 3, iters: 800, time: 0.566, data: 0.000) G_GAN: 1.032 G_L1: 21.798 D_real: 0.867 D_fake: 0.099 \n",
      "(epoch: 3, iters: 900, time: 0.569, data: 0.000) G_GAN: 2.594 G_L1: 38.243 D_real: 0.008 D_fake: 0.157 \n",
      "(epoch: 3, iters: 1000, time: 0.586, data: 0.000) G_GAN: 2.057 G_L1: 49.496 D_real: 0.003 D_fake: 0.331 \n",
      "saving the latest model (epoch 3, total_iters 3000)\n",
      "End of epoch 3 / 60 \t Time Taken: 607 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 4, iters: 100, time: 0.571, data: 4.439) G_GAN: 1.325 G_L1: 28.796 D_real: 0.077 D_fake: 0.591 \n",
      "(epoch: 4, iters: 200, time: 0.612, data: 0.000) G_GAN: 0.975 G_L1: 25.390 D_real: 0.520 D_fake: 0.216 \n",
      "(epoch: 4, iters: 300, time: 0.573, data: 0.000) G_GAN: 1.680 G_L1: 46.352 D_real: 0.001 D_fake: 0.302 \n",
      "(epoch: 4, iters: 400, time: 0.577, data: 0.000) G_GAN: 1.782 G_L1: 27.026 D_real: 0.019 D_fake: 0.293 \n",
      "(epoch: 4, iters: 500, time: 0.565, data: 0.000) G_GAN: 1.179 G_L1: 21.772 D_real: 0.330 D_fake: 0.533 \n",
      "(epoch: 4, iters: 600, time: 0.578, data: 0.000) G_GAN: 1.385 G_L1: 12.043 D_real: 1.411 D_fake: 0.096 \n",
      "(epoch: 4, iters: 700, time: 0.573, data: 0.000) G_GAN: 1.943 G_L1: 31.926 D_real: 0.511 D_fake: 0.120 \n",
      "(epoch: 4, iters: 800, time: 0.567, data: 0.000) G_GAN: 0.922 G_L1: 27.278 D_real: 0.782 D_fake: 1.000 \n",
      "(epoch: 4, iters: 900, time: 0.568, data: 0.000) G_GAN: 1.934 G_L1: 28.721 D_real: 0.005 D_fake: 0.257 \n",
      "(epoch: 4, iters: 1000, time: 0.612, data: 0.000) G_GAN: 2.491 G_L1: 42.848 D_real: 0.006 D_fake: 0.291 \n",
      "saving the latest model (epoch 4, total_iters 4000)\n",
      "End of epoch 4 / 60 \t Time Taken: 609 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 5, iters: 100, time: 0.603, data: 3.924) G_GAN: 1.707 G_L1: 34.718 D_real: 0.342 D_fake: 0.368 \n",
      "(epoch: 5, iters: 200, time: 58.081, data: 0.000) G_GAN: 1.201 G_L1: 19.410 D_real: 0.426 D_fake: 0.520 \n",
      "(epoch: 5, iters: 300, time: 0.548, data: 0.000) G_GAN: 1.737 G_L1: 27.401 D_real: 0.001 D_fake: 0.233 \n",
      "(epoch: 5, iters: 400, time: 0.587, data: 0.000) G_GAN: 2.880 G_L1: 36.140 D_real: 0.279 D_fake: 0.108 \n",
      "(epoch: 5, iters: 500, time: 0.548, data: 0.000) G_GAN: 1.634 G_L1: 24.698 D_real: 0.103 D_fake: 0.470 \n",
      "(epoch: 5, iters: 600, time: 0.543, data: 0.000) G_GAN: 1.495 G_L1: 25.143 D_real: 0.247 D_fake: 0.339 \n",
      "(epoch: 5, iters: 700, time: 0.545, data: 0.001) G_GAN: 2.190 G_L1: 28.914 D_real: 0.004 D_fake: 0.182 \n",
      "(epoch: 5, iters: 800, time: 0.557, data: 0.000) G_GAN: 1.448 G_L1: 22.253 D_real: 0.215 D_fake: 1.211 \n",
      "(epoch: 5, iters: 900, time: 0.575, data: 0.000) G_GAN: 2.108 G_L1: 38.196 D_real: 0.001 D_fake: 0.396 \n",
      "(epoch: 5, iters: 1000, time: 0.554, data: 0.000) G_GAN: 1.773 G_L1: 18.312 D_real: 0.568 D_fake: 0.545 \n",
      "saving the latest model (epoch 5, total_iters 5000)\n",
      "saving the model at the end of epoch 5, iters 5000\n",
      "End of epoch 5 / 60 \t Time Taken: 792 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 6, iters: 100, time: 0.563, data: 4.772) G_GAN: 1.717 G_L1: 34.751 D_real: 1.239 D_fake: 0.115 \n",
      "(epoch: 6, iters: 200, time: 0.597, data: 0.000) G_GAN: 2.841 G_L1: 46.816 D_real: 0.006 D_fake: 0.085 \n",
      "(epoch: 6, iters: 300, time: 0.600, data: 0.000) G_GAN: 1.044 G_L1: 38.336 D_real: 1.269 D_fake: 0.343 \n",
      "(epoch: 6, iters: 400, time: 0.601, data: 0.000) G_GAN: 1.150 G_L1: 14.816 D_real: 0.612 D_fake: 0.339 \n",
      "(epoch: 6, iters: 500, time: 0.583, data: 0.000) G_GAN: 1.325 G_L1: 15.465 D_real: 0.118 D_fake: 0.428 \n",
      "(epoch: 6, iters: 600, time: 1.058, data: 0.000) G_GAN: 2.485 G_L1: 38.423 D_real: 0.103 D_fake: 0.063 \n",
      "(epoch: 6, iters: 700, time: 0.717, data: 0.001) G_GAN: 2.823 G_L1: 30.842 D_real: 0.001 D_fake: 0.113 \n",
      "(epoch: 6, iters: 800, time: 0.941, data: 0.001) G_GAN: 1.804 G_L1: 13.262 D_real: 0.083 D_fake: 0.209 \n",
      "(epoch: 6, iters: 900, time: 0.754, data: 0.001) G_GAN: 1.417 G_L1: 18.282 D_real: 0.018 D_fake: 0.603 \n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot ./datasets/mnist/AB \\\n",
    "    --name mnist_pix2pix \\\n",
    "    --model pix2pix \\\n",
    "    --no_flip \\\n",
    "    --gpu_ids -1 \\\n",
    "    --display_id -1 \\\n",
    "    --save_latest_freq 1000 \\\n",
    "    --print_freq 100 \\\n",
    "    --max_dataset_size 1000 \\\n",
    "    --n_epochs 30 \\\n",
    "    --n_epochs_decay 30 #\\\n",
    "    # --epoch_count 8\n",
    "    # --continue_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCsKkEq0yGh0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./datasets/mnist/AB           \t[default: None]\n",
      "             dataset_mode: aligned                       \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: -1                            \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \t[default: test]\n",
      "               n_layers_D: 3                             \n",
      "                     name: mnist_pix2pix                 \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                     norm: batch                         \n",
      "                 num_test: 50                            \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "dataset [AlignedDataset] was created\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "loading the model from ./checkpoints/mnist_pix2pix/latest_net_G.pth\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kevinbeaulieu/projects/pytorch-CycleGAN-and-pix2pix/test.py\", line 52, in <module>\n",
      "    model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kevinbeaulieu/projects/pytorch-CycleGAN-and-pix2pix/models/base_model.py\", line 89, in setup\n",
      "    self.load_networks(load_suffix)\n",
      "  File \"/Users/kevinbeaulieu/projects/pytorch-CycleGAN-and-pix2pix/models/base_model.py\", line 203, in load_networks\n",
      "    state_dict = torch.load(load_path, map_location=str(self.device))\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kevinbeaulieu/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/serialization.py\", line 998, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kevinbeaulieu/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/serialization.py\", line 445, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kevinbeaulieu/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/serialization.py\", line 426, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/mnist_pix2pix/latest_net_G.pth'\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataroot ./datasets/mnist/AB \\\n",
    "    --model pix2pix \\\n",
    "    --name mnist_pix2pix \\\n",
    "    --gpu_ids -1\n",
    "    # --use_wandb \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "pix2pix",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
